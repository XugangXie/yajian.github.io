<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"><style>.pace .pace-progress{background:#1E92FB;height:3px}.pace .pace-progress-inner{box-shadow:0 0 10px #1E92FB,0 0 5px #1E92FB}.pace .pace-activity{border-top-color:#1E92FB;border-left-color:#1E92FB}</style><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="iuqAv5dkTjkC-tYrbkff9KGsw2tnVjzlEHIubXlnmaI"><meta name="baidu-site-verification" content="FKXU75MFl1"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="icon" type="image/png" sizes="32x32" href="/images/iris_32*32.ico?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/iris_16*16.ico?v=5.1.3"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="小建儿的小站" type="application/atom+xml"><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement(o),n=t.getElementsByTagName(o)[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,"script",("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"84485d15"}),daovoice("update")</script><meta name="description" content="码农小白成长记"><meta property="og:type" content="website"><meta property="og:title" content="小建儿的小站"><meta property="og:url" content="http://yajian.github.io/page/5/index.html"><meta property="og:site_name" content="小建儿的小站"><meta property="og:description" content="码农小白成长记"><meta property="og:locale" content="zh-Hans"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="小建儿的小站"><meta name="twitter:description" content="码农小白成长记"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yajian.github.io/page/5/"><title>小建儿的小站</title></head><body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">小建儿的小站</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tag"></i><br> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/ANTLR简介/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/ANTLR简介/" itemprop="url">ANTLR简介</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-16T18:06:24+08:00">2018-03-16</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/ANTLR简介/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="ANTLR简介/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;目前AI领域很多开源框架都朝着低门槛的方向发展，以keras为例，极大地简化了tensorflow的使用。我们的项目同样面临这个问题，为了降低使用门槛，需要使用解析器把易于常人理解的配置转化为复杂的后端执行代码。在调研过程中，遇到了有很多表达式解析工具，如ik-expression、aviator等，最后我学习spark-sql的时候了解了spark2.0底层的sql解析器使用了ANTLR ，后来发现ANTLR 是一款优秀的工业级语法分析器，所以决定用到我们的项目中。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>语言：一门语言是一个有效语句的集合，语句(sentence)由词组组成，词组(phrase)由子词组组成，子词组(subphrase)又由更小的词组组成，依次类推；</li><li>语法：语法定义了语言的语义规则，语法中的每条规则定义了一种词组结构；</li><li>语义树或语法分析树：代表了语句的结构，其中的每个子树的根节点都使用一个抽象的名字给包含的元素命名。即子树的根节点对应了语法规则的名字。树的叶子结点是语句的符号或者词法的符号；</li><li>词法符号：词法符号是一门语言的基本词汇符号，他们可以代表像是“标志符”这样的一类符号，也可以代表一个单一的运算符，或者代表一个关键字；</li><li>词法分析器或者词法符号生成器：将字符聚集为单词或者符号(token)的过程称为词法分析(lexical analysis)或者词法符号化(tokenzing)。我们可以把输入文本转换为词法符号的程序称为词法分析器(lexer)。词法符号包含至少两部分信息：词法符号类型和词法符号对应的文本；</li><li>语法分析器：识别语言的程序叫做语法分析器(parser)或者句法分析器(syntax analyzer)。句法(syntax)是指约束语言中的各个组成部分之间关系的规则。语法(grammar)是一系列规则的集合，每条规则表述出一种词汇结构。语法分析器通过检查语句的结构是否符合语法规则的定义来验证该语句在特定语言中是否合法；</li><li>递归下降的语法分析器：这事自顶向下的语法分析器的一种实现，每条规则都对应语法分析器中的一个函数；</li><li>前向预测：语法分析器使用前向预测来进行决策，具体方法是：将输入符号与每个备选分支的起始符号进行比较；</li><li>解释器：如果一个程序能够分析计算或者“执行”语句，我们就称之为“解释器”(interpreter)，例子有计算器、读取配置文件的程序或者python解释器；</li><li>翻译器：如果一个程序能够将一门语言的语句转换为另一门语言的语句，我们称之为翻译器(translator)，这样的例子包括java到C#的转换器和普通编译器；</li></ul></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/raspberry初始篇/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/raspberry初始篇/" itemprop="url">raspberry初始篇</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-07T21:58:05+08:00">2018-01-07</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/raspberry/" itemprop="url" rel="index"><span itemprop="name">raspberry</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/raspberry初始篇/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="raspberry初始篇/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;之前就在一些论坛上看到过树莓派相关的技术贴，一直没机会玩，碰巧在今年公司圣诞节活动上得到了一块raspberry pi 3 model b开发版，配齐了显示器、摄像头、云台等配件，想先从机器视觉入手做些小项目。配置raspberry的时候踩到了一些坑，下面详细介绍一下从拆箱到能运行opencv获取视频流的过程。</p><p>##安装系统<br>&emsp;&emsp;raspberry pi 3 model b很小巧，只有巴掌大小，最基本的套餐只有板子，其他的配件都不带。电源部分需要自己配一个安卓充电器和数据线，用之前安卓手机的就可以；存储部分需要准备一张sd卡，一样从原来的安卓手机上拆。接下来就是烧录系统，跟着官网的[document](<a href="https://www.raspberrypi.org/documentation/installation/installing-images/README.md)做，首先从[官网](https://www.raspberrypi.org/downloads/raspbian/)下载RASPBIAN" target="_blank" rel="noopener">https://www.raspberrypi.org/documentation/installation/installing-images/README.md)做，首先从[官网](https://www.raspberrypi.org/downloads/raspbian/)下载RASPBIAN</a> STRETCH WITH DESKTOP系统，文件1.6G，建议用utorrent下载。下载好系统后，制作启动盘，我用的是mac，所以下载[Etcher](<a href="https://etcher.io/)。Etcher的使用很简单，跟着官方使用方法3步就可以把系统烧到sd卡中，然后上电启动，raspberry就可以正常运行了。" target="_blank" rel="noopener">https://etcher.io/)。Etcher的使用很简单，跟着官方使用方法3步就可以把系统烧到sd卡中，然后上电启动，raspberry就可以正常运行了。</a></p><p>##触屏校正<br>&emsp;&emsp;建议买一块带触摸功能的屏幕，没有键盘的时候还能进行简单的点击操作，装个虚拟键盘可以打字。屏幕是淘宝的3.5寸HDMI电阻屏，附上淘宝[链接](<a href="https://item.taobao.com/item.htm?spm=a1z09.2.0.0.5345c157cwgnrR&amp;id=553530372753&amp;_u=71p5273o0b4a)，里面可以找到具体的屏幕参数。在没有安装驱动之前，显示的分辨率和触屏功能都是有问题的，首先要安装驱动，链接里也有详细说明，这里贴一下。" target="_blank" rel="noopener">https://item.taobao.com/item.htm?spm=a1z09.2.0.0.5345c157cwgnrR&amp;id=553530372753&amp;_u=71p5273o0b4a)，里面可以找到具体的屏幕参数。在没有安装驱动之前，显示的分辨率和触屏功能都是有问题的，首先要安装驱动，链接里也有详细说明，这里贴一下。</a></p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf LCD-<span class="keyword">show</span></span><br><span class="line"></span><br><span class="line">git clone https:<span class="comment">//github.com/goodtft/LCD-show.git</span></span><br><span class="line"></span><br><span class="line">chmod -R <span class="number">755</span> LCD-<span class="keyword">show</span></span><br><span class="line"></span><br><span class="line">cd LCD-<span class="keyword">show</span>/</span><br><span class="line"></span><br><span class="line">sudo ./MPI3508_480_320-<span class="keyword">show</span></span><br></pre></td></tr></table></figure><p>&emsp;&emsp;这步搞完，屏幕的触摸功能依然不好用，左右是颠倒的，点击左边选择的却是右边，需要进行校正。运行xinput_calibrator工具，屏幕四个角会出现十字，按顺序点击，就搞定了触屏校正。具体的参考见[给树莓派装上触摸屏](<a href="http://shumeipai.nxez.com/2013/10/02/raspberry-pi-to-install-touch-screen.html)，跟着指示做，亲测有效。" target="_blank" rel="noopener">http://shumeipai.nxez.com/2013/10/02/raspberry-pi-to-install-touch-screen.html)，跟着指示做，亲测有效。</a><br>&emsp;&emsp;到此，你的树莓派已经可以正常的显示桌面，和正常操作触摸功能了。</p><p>##摄像头安装及监控程序安装<br>&emsp;&emsp;摄像头也是淘宝的，附上链接[摄像头](<a href="https://item.taobao.com/item.htm?spm=a1z09.2.0.0.402d80bcmbDsbn&amp;id=558993629563&amp;_u=81p5273ofec9)，为了之后能跟踪运动物体，建议连云台一起买了。这款摄像头是usb接口的，即插即用，可以使用lsusb查看设备，如果有“Bus" target="_blank" rel="noopener">https://item.taobao.com/item.htm?spm=a1z09.2.0.0.402d80bcmbDsbn&amp;id=558993629563&amp;_u=81p5273ofec9)，为了之后能跟踪运动物体，建议连云台一起买了。这款摄像头是usb接口的，即插即用，可以使用lsusb查看设备，如果有“Bus</a> 001 Device 004: ID 1908:2310 GEMBIRD”就说明设备已经连接了。</p><p>&emsp;&emsp;接下来就是装一款获取视频流的软件，试试摄像头好不好用，最开始我安装的motion，非常卡！！建议安装另一款软件mjpg-streamer，安装这玩意也是费了九牛二虎之力。说说中间遇到的问题，首先去github复制[mjpg-streamer项目](<a href="https://github.com/jacksonliam/mjpg-streamer)，然后make，目前一切正常。然而执行./start.sh就报Init" target="_blank" rel="noopener">https://github.com/jacksonliam/mjpg-streamer)，然后make，目前一切正常。然而执行./start.sh就报Init</a> v4L2 failed !! exit fatal错误，开始查了半天说是不支持YUV编码，那改启动脚本，加上-y参数使用YUV编码，改完之后是这样</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#start</span>.sh</span><br><span class="line"></span><br><span class="line">./mjpg_streamer -<span class="selector-tag">i</span> <span class="string">"./input_uvc.so -y"</span> -o <span class="string">"./output_http.so -w ./www</span></span><br></pre></td></tr></table></figure><p>此时应该好了吧？！再执行还他么不行，继续报Init v4L2 failed !! exit fatal i: init_VideoIn failed。又查了半天才知道是v4l2没有加载到linux内核，执行</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo modprobe bc<span class="name">m2835</span>-v<span class="number">4</span>l<span class="number">2</span></span><br></pre></td></tr></table></figure><p>再次执行启动文件，就ok了。后来又修改了一下/etc/modules-load.d/modules.conf文件，把 bcm2835-v4l2加上去，这样就保险了。</p><p>&emsp;&emsp;程序启动后出现下面的命令说明启动正常，访问ip:8080/stream.html，就可以获取实时图像了，速度很快。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">MJPG Streamer Version.: 2.0</span><br><span class="line"> i: Using V4L2 device.: /dev/video0</span><br><span class="line"> i: Desired Resolution: 640 x 480</span><br><span class="line"> i: Frames Per Second.: -1</span><br><span class="line"> i: Format<span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>: YUYV</span><br><span class="line"> i: JPEG Quality<span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>: 80</span><br><span class="line"> i: TV-Norm<span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>.: DEFAULT</span><br><span class="line"> o: www-folder-path<span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>: ./www/</span><br><span class="line"> o: HTTP TCP port<span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>: 8080</span><br><span class="line"> o: HTTP Listen Address<span class="built_in">..</span>: (<span class="literal">null</span>)</span><br><span class="line"> o: username:password<span class="built_in">..</span><span class="built_in">..</span>: disabled</span><br><span class="line"> o: commands<span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>.: enabled</span><br></pre></td></tr></table></figure><p>##opencv安装<br>&emsp;&emsp;接下来安装opencv，还是先去github下载[opencv](<a href="https://github.com/opencv/opencv)和[opencv_contrib](https://github.com/opencv/opencv_contrib)，然后新建个build文件夹，执行cmake编译" target="_blank" rel="noopener">https://github.com/opencv/opencv)和[opencv_contrib](https://github.com/opencv/opencv_contrib)，然后新建个build文件夹，执行cmake编译</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cmake -D <span class="attribute">CMAKE_BUILD_TYPE</span>=RELEASE \</span><br><span class="line">-D <span class="attribute">CMAKE_INSTALL_PREFIX</span>=/usr/local \</span><br><span class="line">-D <span class="attribute">INSTALL_C_EXAMPLES</span>=ON \</span><br><span class="line">-D <span class="attribute">INSTALL_PYTHON_EXAMPLES</span>=ON \</span><br><span class="line">-D <span class="attribute">OPENCV_EXTRA_MODULES_PATH</span>=../../opencv_contrib/modules \</span><br><span class="line">-D <span class="attribute">BUILD_EXAMPLES</span>=ON <span class="built_in">..</span></span><br></pre></td></tr></table></figure><p>完事正式编译opencv，使用make -j4命令，然后执行到80%多就死机了，每次都是执行了个把小时，真是无语。。。后来发现4核一起会死机！直接make毛事没有，最终编译了过去。附上[参考贴](<a href="http://www.trtos.com/web/wp/?p=1340)。" target="_blank" rel="noopener">http://www.trtos.com/web/wp/?p=1340)。</a></p><p>&emsp;&emsp;摄像头和opencv都准备好了，可以用opencv试试获取实时视频流，写个python脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">clicked = <span class="keyword">False</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onMouse</span><span class="params">(event,x,y,flags,param)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> clicked</span><br><span class="line">    <span class="keyword">if</span> event == cv2.EVENT_LBUTTONUP:</span><br><span class="line">        clicked = <span class="keyword">True</span></span><br><span class="line">cameraCapture=cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line">cv2.namedWindow(<span class="string">'My Window'</span>)</span><br><span class="line">cv2.setMouseCallback(<span class="string">'My Window'</span>,onMouse)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'showing camera feed, Click window or press any key to stop'</span></span><br><span class="line">success,frame = cameraCapture.read()</span><br><span class="line"><span class="keyword">while</span> success <span class="keyword">and</span> cv2.waitKey(<span class="number">1</span>)==<span class="number">-1</span> <span class="keyword">and</span> <span class="keyword">not</span> clicked:</span><br><span class="line">    cv2.imshow(<span class="string">'My Window'</span>,frame)</span><br><span class="line">    success,frame = cameraCapture.read()</span><br><span class="line">cv2.destroyWindow(<span class="string">'My Window'</span>)</span><br><span class="line">cameraCapture.release()</span><br></pre></td></tr></table></figure><p>执行之后就可以获取实时视频流了。</p><p>##总结<br>&emsp;&emsp;环境准备是个大坑，每个人遇到的问题都不一样，这时候就知道了google的好处。不过好歹是把基本环境搭建了起来，之后看看用opencv做点小项目玩玩，有新进展将继续更新。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/机器学习策略-1/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/机器学习策略-1/" itemprop="url">机器学习策略-1</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-03T20:20:33+08:00">2017-12-03</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/机器学习策略-1/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="机器学习策略-1/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;本周视频讲解了在机器学习实践中会用到的一些基本的策略，整理了个人比较重要的一些内容。</p><h2 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h2><p>&emsp;&emsp;正交化是一种系统设计属性，是指确保修改算法的指令或者组件时，不会产生或传播副作用到系统的其他组件中。正交系统模块间互相依赖较小，可以相互独立地进行算法验证，能够有效减少开发和测试时间。<br>&emsp;&emsp;当设计一个监督学习系统时，需要做到下面的4个假设并且是相互正交的：</p><ol><li>模型在训练集上表现良好</li><li>模型在验证集上表现良好</li><li>模型在测试集上表现良好</li><li>模型在实际应用中表现良好</li></ol><p>&emsp;&emsp;当发现训练集上表现不够好时，可以采用更大的神经网络或者换一种更好的优化算法；当在验证集上表现不够好时，可以进行正则化处理或者加入更多的训练数据；在测试集上效果不够好时，可以采用更大的验证集进行验证；当在实际应用中表现不好时，可能是因为没有正确设置测试集或者代价函数评估出现问题。</p><h2 id="单一数字评估指标"><a href="#单一数字评估指标" class="headerlink" title="单一数字评估指标"></a>单一数字评估指标</h2><p>&emsp;&emsp;在构建机器学习系统时，通过设置单一数字评估指标，可以更快更好地评估模型。对于二分类问题，常用的评价指标是精准率(Precision)和召回率(Recall)。将所关注的类作为正类(Positive)，其他类作为负类(Negative)，并根据分类器在数据集上预测的正确与否，有以下4种情况：</p><ul><li>TP(True Positive) –正类预测为正类</li><li>FN(False Negative) –正类预测为负类</li><li>FP(False Positive) –负类预测为正类</li><li>TN(True Positive) –负类预测为负类</li></ul><p>所以有，精准率为<br>$$P=\frac{TP}{TP+FP}$$</p><p>召回率为</p><p>$$R=\frac{TP}{TP+FN}$$</p><p>&emsp;&emsp;当遇到下面的情况时，难以分辨哪个模型更优，就需要使用F1 Score指标。</p><div align="center"><br> <img src="/机器学习策略-1/模型评估指标.jpg" width="300" height="100" align="center" title="模型评估指标"><br></div><p>F1 Score定义为<br> $$F_1=\frac{2}{\frac{1}{P}+\frac{1}{R}}=\frac{2TP}{2TP+FP+FN} $$<br>F1 Score是精准率和召回率的调和平均数，比简单的去平均数效果要好。</p><h2 id="满足和优化指标"><a href="#满足和优化指标" class="headerlink" title="满足和优化指标"></a>满足和优化指标</h2><p>&emsp;&emsp;有时候，判断的标准不限于一个单一数字的评估指标。假设有三个不同的分类器性能表现如下：</p><p></p><div align="center"><br> <img src="/机器学习策略-1/优化指标.jpg" width="450" height="150" align="center" title="满足和优化指标"><br></div><br>如要求模型准确率尽可能的高，运行时间在100 ms以内。这里以Accuracy为优化指标，以Running time为满足指标，可以从中选出B是满足条件的最好的分类器。<p></p><h2 id="训练、开发、测试集"><a href="#训练、开发、测试集" class="headerlink" title="训练、开发、测试集"></a>训练、开发、测试集</h2><p>&emsp;&emsp; 一般把收集到的现有数据分为训练集、验证集和测试集。构建机器学习系统时，在训练集上训练出不同的模型，随后使用验证集对模型的好坏进行评估，确信某个模型效果足够好时再用测试集进行测试。<br>&emsp;&emsp; 验证集和测试集的来源应该是相同的，并且是从所有数据中随机抽取；其次注意数据集大小的划分，原来的分配经验是将数据的70%作为训练集30%作为测试集，或者60%作为训练集20%作为验证集20%作为测试集。当数据量较少时，这种划分方式是合理的。但是在大数据量的场景下，一般不遵守该原则，测试集的大小应该设置得足够提高系统整体性能得可信度，开发集的大小也要设置得足够用于评估几个不同的模型。</p><h2 id="改变开发、测试集和评估指标"><a href="#改变开发、测试集和评估指标" class="headerlink" title="改变开发、测试集和评估指标"></a>改变开发、测试集和评估指标</h2><p>&emsp;&emsp;在针对某一问题设置好验证集和评估指标后，不是一成不变的，有以后会发现目标设置错误，所以需要改动开发、测试集或评估指标。<br>&emsp;&emsp;假设有两个猫的图片分类器，评估指标为错误率，模型A的错误了为3%，模型B的错误率为5%。表面上看A的效果更好。但是在实际应用中，A可能会将很多色情图片分类成了猫。所以当在线上部署的时候，算法A会给爱猫人士推送更多更准确的猫的图片，但同时也会给用户推送一些色情图片，这是不能忍受的(为啥我觉得A更好，可能是因为我不是爱猫人士。。。。)。所以，虽然算法A的错误率很低，但是它却不是一个好的算法。这时我们需要改变验证集和测试集或者评估指标。<br>假设开始的评估指标如下：<br>$$E=\frac{1}{m_{dev}}\sum_{i=1}^{m_{dev}}I({y_{pred}^{(i)}\not=y^{(i)}})$$<br>该指标没有区分普通非猫图片和色情图片的误差，但是实际中，我们会希望当把色情图片标记为猫的时候误差更大一些，于是加入权重项$w^{(i)}$<br>$$E=\frac{1}{\sum w^{(i)}}\sum_{i=1}^{m_{dev}}w^{(i)}I({y_{pred}^{(i)}\not=y^{(i)}})$$<br>当$x^{(i)}$是色情图片时，$w^{(i)}=10$，否则$w^{(i)}=1$，以此来区分色情图片及其他误识别的图片。所以，要根据实际情况，正确确定一个评判指标，确保这个评判指标最优。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/改善深层神经网络：超参数调试、正则化以及优化/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/改善深层神经网络：超参数调试、正则化以及优化/" itemprop="url">改善深层神经网络：超参数调试、正则化以及优化</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-24T19:27:53+08:00">2017-11-24</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/改善深层神经网络：超参数调试、正则化以及优化/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="改善深层神经网络：超参数调试、正则化以及优化/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;这周的视频主要讲了超参数搜索方法、Batch Norm归一化处理、SoftMax回归以及TensorFlow计算框架的使用。</p><h2 id="搜索超参数"><a href="#搜索超参数" class="headerlink" title="搜索超参数"></a>搜索超参数</h2><p>&emsp;&emsp;在之前的课程中出现了很多超参数，如学习率$a$、神经网络层数$l$，在求解过程中需要对超参数的取值进行调整。超参数的调整可以遵循以下几个规则</p><ol><li>随机搜索优于网格搜索，在进行网格搜索时，同一个纬度取的值较少，如下图，同样是取25个值，在进行网格搜索时超参数1只使用了5个值，而进行随机搜索时参数1和参数2都使用了25个值。</li></ol><div align="center"><br> <img src="/改善深层神经网络：超参数调试、正则化以及优化/网格搜索和随机搜索.jpg" width="500" height="250" align="center" title="网格搜索和随机搜索"><br></div><ol start="2"><li>由粗略搜索到精细搜索，即先在大范围内寻找效果好的区域范围，再在该范围内进行精细搜索</li><li>第1条中说的随机搜索并不是在有效范围内均匀随机取值，而是需要选择合适的坐标系。对于隐藏层数来说，可以在规定范围内进行均匀随机取值，如范围时2到4层，取值时选取2、3、4三个值。但是对于学习率α，其范围在0.0001到1之间，如果α有很大可能在0.0001到0.1之间，若随机均匀取值，那么在0.1到1之间将用去90%的资源，这是不合理的。这时候要是取对数坐标，就能在0.0001到0.1之间取得更多的值。</li></ol><p>&emsp;&emsp;为了优化模型效果，通常有两种方式进行训练。第一种是选择一个模型进行训练，观察其代价函数，在其变化的过程中，不断调节超参数，使曲线不上升，这种方法适用于数据量大、时间充足的情况；第二种是同时建立多个模型，观察所有模型的代价函数，选择表现较好的模型，这种方法适用于计算能力较强的情况。</p><h2 id="Batch-Norm归一化处理"><a href="#Batch-Norm归一化处理" class="headerlink" title="Batch Norm归一化处理"></a>Batch Norm归一化处理</h2><p>&emsp;&emsp;在视频中Ng把输入层归一化推广到隐含层归一化，并且从前向传播角度说明了Batch Norm的好处。其实可以从公式推导角度证明Batch Norm为什么可以加快训练速度。<br>&emsp;&emsp;当使用sigmoid函数作为激活函数时，其导数在x=0时取得最大值，当x远离0时其导数基本为0。而对于正态分布而言，有95%的数据在[-2，2]的范围内，这样在进行反向传播时出现梯度消失的数据较少，使得整体上保持较快速度收敛。如果不进行Batch Norm处理，数据就会落在激活函数的饱和区，这样梯度越来越小，甚至出现梯度消失现象。</p><p></p><div align="center"><br> <img src="/改善深层神经网络：超参数调试、正则化以及优化/标准正态分布.jpg" width="350" height="250" align="center" title="标准正态分布"><br></div><br>&emsp;&emsp;下面说说Batch Norm的处理过程<br>$$\mu=\frac{1}{m}\sum_{i}^{}z^{(i)}$$<br>$$\sigma^2=\frac{1}{m}\sum_{i}^{} (z^{(i)}-\mu)^2$$<br>$$z^{(i)}_{norm}=\frac{z^{(i)-\mu}}{\sqrt{\sigma^2+\varepsilon}}$$<br>$${z^{(i)}_{norm}}^{‘} = \gamma z^{(i)}_{norm}+\beta$$<br>最后又加入了$\gamma$、$\beta$参数，同样可以通过梯度下降进行求解。<p></p><h2 id="SoftMax回归"><a href="#SoftMax回归" class="headerlink" title="SoftMax回归"></a>SoftMax回归</h2><p>&emsp;&emsp;当进行二分类时，最后一层的激活函数可以采用sigmoid函数，当涉及多分类时就要用到softmax函数。先给出softmax函数的形式</p><p>$$h_{\theta}(x^{(i)})=<br>\begin{bmatrix}<br>p(y^{(i)}=1|x^{(i)};\theta)\\<br>p(y^{(i)}=2|x^{(i)};\theta)\\<br>\vdots\\<br>p(y^{(i)}=3|x^{(i)};\theta)\\<br>\end{bmatrix}=<br>\frac{1}{\sum_{j=1}^{k}e^{\theta^{T}_{j}x^{(i)}}}<br>\begin{bmatrix}<br>e^{\theta^{T}_{1}x^{(i)}}\\<br>e^{\theta^{T}_{2}x^{(i)}}\\<br>\vdots\\<br>e^{\theta^{T}_{k}x^{(i)}}\\<br>\end{bmatrix}<br>$$<br>举个视频的例子就是</p><p></p><div align="center"><br> <img src="/改善深层神经网络：超参数调试、正则化以及优化/softmax回归例子.jpg" width="400" height="250" align="center" title="softmax回归例子"><br></div><br>&emsp;&emsp;softmax回归做了两件事情，第一步是将模型的每个输出加上了指数映射，将其取值范围定义在正实数，第二步是把经过转换的输出进行归一化，得到概率分布。在网上给出的推导版本中都是直接给出了softmax的指数形式，其实线性回归、logistic回归、softmax回归都是从广义线性模型推导出来的，当y的分布是正态分布时得到的就是线性模型，当y是两点分布时得到的是logistic回归模型，当y是多项式分布时得到的是softmax回归模型。<p></p><h2 id="TensorFlow的使用"><a href="#TensorFlow的使用" class="headerlink" title="TensorFlow的使用"></a>TensorFlow的使用</h2><p>&emsp;&emsp;在进行神经网络搭建事，个人觉得还是keras比较好用，接口抽象的比较友好，不过既然视频提到了TensorFlow，那也研究一下吧，贴上代码</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line">import tensorflow as tf </span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="comment">#准备数据</span></span><br><span class="line">train_X = np.linspace(-1,1,100)</span><br><span class="line">train_Y = 2 * train_X + np.random.randn(*(train_X.shape))*0.33 + 10</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">w = tf.Variable(0.0,name=<span class="string">"weight"</span>)</span><br><span class="line">b = tf.Variable(0.0,name=<span class="string">"bias"</span>)</span><br><span class="line"><span class="comment">#定义均方误差损失函数</span></span><br><span class="line">loss = tf.square(Y - tf.multiply(X,w) -b)</span><br><span class="line">tran_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型训练</span></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer())</span><br><span class="line">	epoch =1</span><br><span class="line">	for i in range(10):</span><br><span class="line">		for (x,y) in zip(train_X,train_Y):</span><br><span class="line">			_,w_value,b_value = sess.run([tran_op,w,b],feed_dict=&#123;X:x,Y:y&#125;)</span><br><span class="line">			print <span class="string">"Epoch:&#123;&#125;, w:&#123;&#125;, b:&#123;&#125;"</span>.format(epoch,w_value,b_value)</span><br><span class="line">			epoch+=1</span><br></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:1</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-0.157984390855</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:0.157984390855</span></span><br><span class="line">  </span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:2</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-0.311834096909</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:0.315006256104</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:3</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-0.45063239336</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:0.459648668766</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:4</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-0.587551951408</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:0.605401754379</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:5</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-0.719821453094</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:0.749299347401</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:6</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-0.839450538158</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:0.882369875908</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:7</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-0.967249691486</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:1.02779650688</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:8</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-1.08310699463</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:1.16273617744</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:9</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:-1.18981790543</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:1.29001784325</span></span><br><span class="line">...</span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:990</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.9911210537</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0346708298</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:991</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.99132072926</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0349149704</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:992</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.9895298481</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0327787399</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:993</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.99043321609</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0338306427</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:994</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.99787294865</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0422964096</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:995</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.98872160912</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.03211689</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:996</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.99850010872</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.042755127</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:997</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.99492764473</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0389518738</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:998</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.99669957161</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0407981873</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:999</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:1.99851024151</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0426464081</span></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">Epoch</span><span class="selector-pseudo">:1000</span>, <span class="selector-tag">w</span><span class="selector-pseudo">:2.01060414314</span>, <span class="selector-tag">b</span><span class="selector-pseudo">:10.0547399521</span></span><br></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/优化算法/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/优化算法/" itemprop="url">优化算法</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-18T13:10:49+08:00">2017-11-18</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/优化算法/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="优化算法/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;这周的视频主要讲了在使用梯度下降求解$w$和$b$时用到的优化方法，主要包括Mini-batch、指数加权平均、动量梯度下降法、RMSprop、Adam优化算法、学习率衰减。这些都是比较实用的方法，在《机器学习实战》中也都有介绍。</p><h2 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h2><p>&emsp;&emsp;说到Mini-batch有必要提一下梯度下降和随机梯度下降。以线性回归为例进行说明，假设$(x_{j},y_{j})$是输入输出向量，参数个数为$n$，样本个数为$m$，$h(x)$是预测函数，$J(\theta)$为损失函数，这里取平方误差函数，则有<br>$$h(\theta)=\sum_{j=0}^{n}\theta_{j}x_{j}$$<br>$$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(y^{i}-h_{\theta}(x^{i}))^2$$</p><h3 id="批量梯度下降（Batch-Gradient-Descent）"><a href="#批量梯度下降（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降（Batch Gradient Descent）"></a>批量梯度下降（Batch Gradient Descent）</h3><p>&emsp;&emsp;批量梯度下降首先对损失函数$J(\theta)$求导<br>$$\frac{\partial{J(\theta)}}{\partial{\theta_{j}}}=-\frac{1}{m}\sum_{i=1}^{m}(y^{i}-h_{\theta}(x^{i}))x^{i}_{j}$$<br>要最小化损失函数，所以每个参数都要沿着负梯度的方向进行更新，有<br>$$\theta_{j}=\theta_{j}+\eta*\frac{1}{m}\sum_{i=1}^{m}(y^{i}-h_{\theta}(x^{i}))x^{i}_{j}$$<br>从上面的公式中可以看出，每进行一次更新都需要用到$m$个样本，即最小化所有训练样本的损失函数，最后求得的解是全局最优解。但是当$m$很大时$\theta$更新速度会很慢。</p><h3 id="随机梯度下降（Stochastic-Gradient-Descent）"><a href="#随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent）"></a>随机梯度下降（Stochastic Gradient Descent）</h3><p>&emsp;&emsp;为了避免$m$很大时$\theta$更新速度慢的情况，每次更新只使用一个样本，最小化每条样本的损失函数，当样本量很大时也能利用少量数据求得$\theta$最优解。但是不是每次迭代都能向着全局最优解的方向，若遇上噪声容易陷入局部最优。</p><p>&emsp;&emsp;而Mini-batch是在BGD和SGD中取折中，一次取一部分数据进行梯度更新，既保证了速度，又能让迭代方向与全局最优解的方向保持一致。</p><h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p>&emsp;&emsp;在实际应用中有时候会对原始数据进行处理，降低一些奇异值的影响，达到平滑的目的。Ng用温度的例子比较名确的说明了指数加权平均的处理过程，假设$\theta_{t}$代表第$t$天的温度，$v_{t}$代表第$t$天经过指数加权平均处理的温度，则有<br>$$v_{t} = \beta*v_{t-1}+(1-\beta)\theta_{t}$$<br>其中$\beta$是可调节的超参数。乍看上去这个公式和指数没什么关系，展开来看，让$、beta=0.9$<br>$$v_{100}=0.9v_{99}+0.1\theta_{100}$$<br>$$v_{99}=0.9v_{98}+0.1\theta_{99}$$<br>$$v_{98}=0.9v_{97}+0.1\theta_{98}$$<br>$$……$$<br>所以有$$v_{100}=0.1\theta_{100}+0.1*0.9\theta_{99}+0.1*0.9^2\theta_{98}+……+0.1*0.9^{99}*\theta_{1}$$<br>不难看出本质上就是以指数形式的递减加权的移动平均，各数值的加权随着时间指数级递减。</p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>&emsp;&emsp;在上面提到的梯度下降算法中，$w$和$b$的更新公式是这样的<br>$$w = w-\eta dw$$<br>$$b = b-\eta db$$<br>而动量梯度下降是按照下面的方式进行更新的<br>$$v_{dw}=\beta v_{dw}+(1-\beta)* dw$$<br>$$w=w-\eta v_{dw}$$<br>$$v_{db}=\beta v_{db}+(1-\beta)* db$$<br>$$b=w-\eta v_{db}$$<br>动量梯度下降是将梯度下降的过程加了一个指数加权平均，这样能更好的控制梯度更新的方向。</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>&emsp;&emsp; RMSprop与动量梯度下降算法很想，其$w$和$b$的更新公式如下<br>$$v_{dw}=\beta v_{dw}+(1-\beta)（dw)^2$$<br>$$w=w-\eta \frac{dw}{\sqrt{v_{dw}}}$$<br>$$v_{db}=\beta v_{db}+(1-\beta)(db)^2$$<br>$$b=w-\eta \frac{db}{\sqrt{v_{db}}}$$<br>这样做是防止在对$w$进行迭代时，$w$的变化幅度很小，而$b$的变化很大。当$w$很小时，$\frac{dw}{\sqrt{v_{dw}}}$这项会将$w$变化幅度增大。对于$b$的迭代道理一样。</p><h2 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h2><p>&emsp;&emsp;Adam算法是把Momentum和RMSprop结合，其$w$和$b$的更新公式如下<br>$$v_{dw}=\beta_{1} v_{dw}+(1-\beta_{1})dw$$<br>$$s_{dw}=\beta_{2} s_{dw}+(1-\beta_{2})(dw)^2$$<br>$$v_{dw}^{correct}=\frac{v_{dw}}{(1-\beta_{1}^{t})}$$<br>$$s_{dw}^{correct}=\frac{s_{dw}}{(1-\beta_{2}^{t})}$$<br>$$w=w-\eta \frac{v_{dw}^{correct}}{\sqrt{s_{dw}^{correct}}+\varepsilon}$$<br>$$v_{db}=\beta_{1} v_{db}+(1-\beta_{1})db$$<br>$$s_{db}=\beta_{2} s_{db}+(1-\beta_{2})(db)^2$$<br>$$v_{db}^{correct}=\frac{v_{db}}{(1-\beta_{1}^{t})}$$<br>$$s_{db}^{correct}=\frac{s_{db}}{(1-\beta_{2}^{t})}$$<br>$$b=b-\eta \frac{v_{db}^{correct}}{\sqrt{s_{db}^{correct}}+\varepsilon}$$</p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>&emsp;&emsp;在算法迭代过程中，学习率较大会造成震荡，这时候需要降低学习率。一般会这样设置学习率<br>$$\eta = \eta * \frac{1}{1+decay*epoch_num}$$<br>其中，decay的范围是[0，1]，epoch_num是迭代的次数，所以$\eta$会随着迭代次数的增加而减小。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/HHBK使用技巧/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/HHBK使用技巧/" itemprop="url">HHBK使用技巧</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-14T13:26:36+08:00">2017-11-14</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/小技巧/" itemprop="url" rel="index"><span itemprop="name">小技巧</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/HHBK使用技巧/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="HHBK使用技巧/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;媳妇给买了把HHKB type-s无刻，用了两天感觉确实是退烧良药，打字停不下来。无刻版在打英文的时候毫无压力，但是在输入数字和一些符号的时候有点迷糊，所以还是建议替换数字区的“5”和“0”号键，加个标示。mac下常用的一些快捷键组合要熟记。</p><ol><li>Ctrl+H:退格键</li><li>Ctrl+D:删除键</li><li>Ctrl+B:向左</li><li>Ctrl+F:向右</li><li>Ctrl+N:向下</li><li>Ctrl+P:向上</li><li>Ctrl+A:Home键</li><li>Ctrl+E:End键</li></ol><p>&emsp;&emsp;目前发现一个问题，如果电脑进入休眠一段时间后，键盘无法激活电脑，即使用电脑自带的键盘激活之后，HHKB也无法打字，但是对于无线鼠标来说没有这个问题。在网上搜了一下，出现这个问题的人不多，也没有解决方法。问了淘宝旗舰店的客服，也说很少出现这个问题。现在只能重新插拔键盘，比较郁闷。。。。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/正则化与梯度/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/正则化与梯度/" itemprop="url">正则化与梯度</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-12T18:44:10+08:00">2017-11-12</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/正则化与梯度/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="正则化与梯度/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;课程01《神经网络和深度学习》的学习告一段落，这周开始课程02《改善深层神经网络：超参数调试、正则化及优化》的学习，第一周课程讲述了数据集的划分、方差与偏差的概念、正则化、梯度消失和梯度爆炸等知识点，都是在实际应用中会遇到的一些问题。</p><h2 id="数据集的划分"><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h2><p>&emsp;&emsp;首先明确一下训练集、验证集和测试集的概念，之前很少强调验证集，导致把验证集和测试集的概念混淆。</p><h3 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h3><p>&emsp;&emsp;训练集（training set）用来构建模型、确定模型参数的样本。</p><h3 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h3><p>&emsp;&emsp;验证集（validation set）用于模型优化、确定网络结构或者控制模型复杂程度的参数，主要是辅助模型构建。</p><h3 id="测试集"><a href="#测试集" class="headerlink" title="测试集"></a>测试集</h3><p>&emsp;&emsp;测试集（training set）用于检验训练好的模型的预测能力。<br>&emsp;&emsp;以前没有仔细区分验证集和测试集，一些demo也只是将数据集划分为training set和test test，此处需要注意验证集的基本概念。ng在视频中说对一般量级的数据进行划分的时候，可以取训练集：验证集：测试集=3:1:1的比例；如果分为两类，一般是训练集：测试集=7:3。在大数量级数据的情况下，验证集和测试集占的比例可能较小。ng特意强调了保证测试集和验证集来自同一分布</p><h2 id="方差与偏差"><a href="#方差与偏差" class="headerlink" title="方差与偏差"></a>方差与偏差</h2><h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>&emsp;&emsp;方差描述的是预测值相对于期望值的波动范围，也可以说是离散程度。方差越大，数据的分布越分散，波动范围越大；反之亦然。</p><h3 id="偏差"><a href="#偏差" class="headerlink" title="偏差"></a>偏差</h3><p>&emsp;&emsp;描述的是预测值的期望与真实值之间的差距。偏差越大，和真实值差的越多。</p><p>下图比较形象的说明了两者的区别</p><div align="center"><br> <img src="/正则化与梯度/方差vs偏差.jpg" width="300" height="300" align="center" title="方差vs偏差"><br></div><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>&emsp;&emsp;当模型在训练集的效果在训练集上效果表现很好，但是在测试集上表现很差的话，说明出现了过拟合的情况。举个例子来说，就好比上学的时候做练习题，如果一个学生练习题做的很好，但是考试好不过，说明学习方法有问题，不能很好的掌握所学知识，出现了过拟合的问题。解决过拟合的问题有以下几种方法：数据集扩增、early stopping、正则化。</p><h3 id="数据集扩增"><a href="#数据集扩增" class="headerlink" title="数据集扩增"></a>数据集扩增</h3><p>&emsp;&emsp;对抗过拟合的一个有效方法就是用更多的训练数据去训练模型，用于训练模型的数据多了，说明模型使用范围更广泛。就好比一个学生做的练习题多了，把知识都融会贯通了，考试自然就能考高分。<br>&emsp;&emsp;数据集扩增不太容易办到，ng讲了几个例子可以较方便的扩展数据集，主要包括产生镜像图片、旋转原图、图像加噪声等。</p><h3 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h3><p>&emsp;&emsp;随着代价函数的不断减小，训练误差是一个不断变小的过程，验证误差起初也会不断减小，但是当出现过拟合的时候验证误差会慢慢增加，eraly stopping就是在验证误差即将增大的时候停止迭代的过程。下图能比较明显的表示这个过程。</p><div align="center"><br> <img src="/正则化与梯度/early_stopping.jpg" width="300" height="200" align="center" title="early_stopping"><br></div><h3 id="正则化-1"><a href="#正则化-1" class="headerlink" title="正则化"></a>正则化</h3><p>&emsp;&emsp;正则化方法是指在进行目标函数优化时，在目标函数后面加上一个正则项，来达到防止过拟合的目的，即$C = J(W,b)+R$，其中$J(W,b)$是原损失函数，$R$是正则项。常见的正则项有L1正则和L2正则。</p><h4 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h4><p>&emsp;&emsp;L1正则化就是在原损失函数基础上加上L1范数，即$C = J(W,b)+\frac{\lambda}{n}\sum^{n}_{i=1}|w_{i}|$。L1正则化对所求权重产生的影响从下面推导的过程看出来，首先求导<br>$$\frac{\partial{C}}{\partial{w}}=\frac{\partial{J(W,b)}}{\partial{w}}+\frac{\lambda}{n}sign(w)$$<br>则梯度更新的过程为<br>$$w:=w-\eta\frac{\partial{J(W,b)}}{\partial{w}}-\eta\frac{\lambda}{n}sign(w)$$<br>&emsp;&emsp;当$w&gt;0$时，更新后的$w$变小；而当$w&lt;0$时，更新后的$w$变大，所以产生的效果就是使得$w$趋近于0。</p><h4 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h4><p>&emsp;&emsp;L1正则化就是在原损失函数基础上加上L1范数，即$C = J(W,b)+\frac{\lambda}{2n}\sum^{n}_{i=1}w_{i}^2$。同样可以通过推导来看L2范数对权重产生的影响，首先求导<br>$$\frac{\partial{C}}{\partial{w}}=\frac{\partial{J(W,b)}}{\partial{w}}+\frac{\lambda}{n}w$$<br>则梯度更新的过程为<br>$$w:=w-\eta\frac{\partial{J(W,b)}}{\partial{w}}-\eta\frac{\lambda}{n}w = （1-\eta\frac{\lambda}{n})w-\eta\frac{\partial{J(W,b)}}{\partial{w}}$$<br>因为$\eta，\lambda，n$都是大于0的，所以更新后的$w$相对于没加L2正则的时候变小了，有缩减系数的效果。</p><h4 id="DropOut正则化"><a href="#DropOut正则化" class="headerlink" title="DropOut正则化"></a>DropOut正则化</h4><p>&emsp;&emsp;Dropout是深度学习正则化的一种方式，它指按照一定的概率将一些神经元暂时从网络中丢弃，被丢弃的神经元不会对后面的计算产生影响，效果如下图。</p><div align="center"><br> <img src="/正则化与梯度/dropout.jpg" width="350" height="200" align="center" title="dropout示意图"><br></div><h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><p>&emsp;&emsp;假设有n层网络，b暂时忽略不计，激活函数为线性激活函数，则一个神经元的输出为$z=Wx$，那么经过n层网络输出为<br>$$y=W^{(n)}W^{(n-1)}W^{(n-2)}……W^{(2)}W^{(1)}x$$<br>所以当$W$的值稍比单位矩阵大，经过指数变换，得到的结果会很大；相应的若$W$的值稍比1小，经过指数变换，得到的结果也会变小。<br>&emsp;&emsp;考虑激活函数的话也会产生类似的效果，所以在搭建神经网络时，要考虑激活函数和权重初值，避免出现梯度消失和梯度爆炸的情况。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>&emsp;&emsp;关于正则化的问题，有两点要说明。首先，对于正则化缩减系数的理解：所求的$w$是特征的权重值，当出现过拟合时，几乎样本中的每个点都在拟合的曲线上，就像下图所示。</p><p></p><div align="center"><br> <img src="/正则化与梯度/过拟合曲线.jpg" width="300" height="200" align="center" title="过拟合曲线示意图"><br></div><br>我们可以看出和二次曲线相比，该图像的其斜率较大。由于自变量的阶数对斜率的影响较小，从而可以推断出斜率变大的主要起因是权重值过大。所以减轻过拟合现象就要控制权重值不能过大，这也是为什么要加上正则项。<br>&emsp;&emsp;第二点就是目前接触到的教材都是告诉我们正则有这样几种方法，但是并没有深究这种方法的理论来源。其实正则项推导和贝叶斯学派有一定的关系，但是我比较疑惑的是添加正则项后的模型效果更好，是否说明贝叶斯学派的理论相比频率学派更优呢？关于如何从贝叶斯学派的理论推导出正则项之后再单开一篇博客进行介绍。<p></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/深层神经网络/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/深层神经网络/" itemprop="url">深层神经网络</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-11T20:55:47+08:00">2017-11-11</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/深层神经网络/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="深层神经网络/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;上篇博文在两层神经网络的基础上进行了正向传播和反向传播的推导，由于网络层数较少，且只考虑了一个样本的情况，使得推导不具有普适性，本章将进行深层神经网络场景下的正向反向传播推导。</p><h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><p>&emsp;&emsp;假设第$l$层网络有$n$个神经元，第$l-1$层网络有$m$个神经元，$w^{(l)}_{ij}$表示第$l$层第$i$个神经元与第$l-1$层第$j$个神经元的连接权重，$b^{(l)}_{i}$表示第$l$层第i个单元的偏置项，$z^{(l)}_{i}$表示第$l$层第$i$个神经元的输入， $a^{(l)}_{i}$表示第$l$层第$i$个神经元的输入，$f(x)$为激活函数，则有<br>$$z^{(l)}_{i} = \sum^{m}_{j=1}w^{(l)}_{ij}a^{(l-1)}_{j}+b^{(l)}_{i}$$<br>$$a^{(l)}_{i}=f[z^{(l)}_{i}]$$<br>前向传播比较简单，公式推导比较简单，接下来将其向量化。<br>$$W^{(l)}=\begin{pmatrix}<br>w^{(l)}_{11} &amp; w^{(l)}_{12} &amp;\cdots &amp; w^{(l)}_{1m}\\\<br>w^{(l)}_{21} &amp; w^{(l)}_{22} &amp;\cdots &amp; w^{(l)}_{2m}\\\<br> \vdots &amp; \vdots &amp; \ddots &amp;\vdots \\\<br>w^{(l)}_{n1} &amp; w^{(l)}_{n2} &amp;\cdots &amp; w^{(l)}_{nm}\\<br>\end{pmatrix}$$</p><p>$$A^{(l-1)}=\begin{bmatrix}<br>a^{(l-1)}_{1}\\<br>a^{(l-1)}_{2}\\<br>\vdots\\<br>a^{(l-1)}_{m}<br>\end{bmatrix}$$</p><p>$$B^{(l)}=\begin{bmatrix}<br>b^{(l)}_{1}\\<br>b^{(l)}_{2}\\<br>\vdots\\<br>b^{(l)}_{m}<br>\end{bmatrix}$$</p><p>$$Z^{(l)}=\begin{bmatrix}<br>z^{(l)}_{1}\\<br>z^{(l)}_{2}\\<br>\vdots\\<br>z^{(l)}_{m}<br>\end{bmatrix}$$<br>所以有<br>$$Z^{(l)}=W^{(l)}A^{(l-1)}+B^{(l)}$$<br>$$A^{(l)}=f[Z^{(l)}]$$</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>&emsp;&emsp;上篇博文中反向传播使用的均方误差作为损失函数，实际上还可以使用交叉熵等其他公式，所以此处使用$J(W,b)$来代表损失函数，根据链式求导法则有：<br>$$\bigtriangledown_{W^{(l)}} J(W,b)=\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{W^{(l)}}}=\delta^{(l)}[A^{(l-1)}]^{T}$$<br>$$\bigtriangledown_{b^{(l)}} J(W,b)=\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{b^{(l)}}}=\delta^{(l)}$$<br>其中$\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}=\delta^{(l)}$，接下来继续求$\delta^{(l-1)}=\frac{\partial{J(W,b)}}{\partial{z^{(l-1)}}}$<br>$$\delta^{(l-1)}=\frac{\partial{J(W,b)}}{\partial{z^{(l-1)}}}=\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}<br>\frac{\partial{z^{(l)}}}{\partial{a^{(l-1)}}}<br>\frac{\partial{a^{(l-1)}}}{\partial{z^{(l-1)}}}<br>=[W^{(l)}]^{T}\delta^{(l)}f’(z^{(l-1)})$$<br>其中$\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}=\delta^{(l)}$，$\frac{\partial{z^{(l)}}}{\partial{a^{(l-1)}}} = W^{(l)}$，$\frac{\partial{a^{(l-1)}}}{\partial{z^{(l-1)}}}=f’(z^{(l-1)})$。所以可以从输出层向输入层逐步计算每一层的$\delta^{(l)}$。</p><h3 id="神经网络工作流程"><a href="#神经网络工作流程" class="headerlink" title="神经网络工作流程"></a>神经网络工作流程</h3><p>&emsp;&emsp;整个神经网络的计算过程如下：</p><ol><li>前向传播，利用前向传播公式计算每一层的输出值$a^{(l)}$<br>$$Z^{(l)}=W^{(l)}A^{(l-1)}+B^{(l)}$$<br>$$A^{(l)}=f[Z^{(l)}]$$</li><li>对输出层计算残差<br>$$\delta^{(l)}=\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}$$</li><li>对其他隐含层计算每一层的残差<br>$$\delta^{(l-1)}=\frac{\partial{J(W,b)}}{\partial{z^{(l-1)}}}=\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}<br>\frac{\partial{z^{(l)}}}{\partial{a^{(l-1)}}}<br>\frac{\partial{a^{(l-1)}}}{\partial{z^{(l)}}}<br>=[W^{(l)}]^{T}\delta^{(l)}f’(z^{(l-1)})$$</li><li>计算各层的偏导数$W^{(l)}$、$b^{(l)}$<br>$$\bigtriangledown_{W^{(l)}} J(W,b)=\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{W^{(l)}}}=\delta^{(l)}[A^{(l-1)}]^{T}$$<br>$$\bigtriangledown_{b^{(l)}} J(W,b)=\frac{\partial{J(W,b)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{b^{(l)}}}=\delta^{(l+1)}$$</li><li>利用梯度下降算法更新每一层的梯度<br> $$W^{(l)}:=W^{(l)}-\eta\bigtriangledown_{W^{(l)}}J(W,b)$$<br> $$b^{(l)}:=b^{(l)}-\eta\bigtriangledown_{b^{(l)}} J(W,b)$$</li></ol><p>这次还是以推导为主，比较抽象，下次将结合实例进行分析，有助于理解神经网络的工作过程。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/浅层神经网络/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/浅层神经网络/" itemprop="url">浅层神经网络</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-29T18:10:00+08:00">2017-10-29</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/浅层神经网络/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="浅层神经网络/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;第三周的课程内容包括单隐层神经网络、如何初始化参数、正向传播、计算导数、梯度下降和反向传播，ng结合简单的两层深度学习模型详细阐述了深度学习的原理，中间的计算过程十分重要，需要大家耐心理解、推导。</p><h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>&emsp;&emsp;“神经元”是深度学习网络中的最基本的单元，其结构如下图所示：</p><div align="center"><br> <img src="/浅层神经网络/神经元模型.jpg" width="400" height="200" align="center" title="神经元模型"><br></div><p>该神经元以$x_{1},x_{2},x_{3}$和截距+1为输入，各分量对应的权重值为$\omega_{1},\omega_{2},\omega_{3},b$，所以该神经元对应的输出为$h_{W,b}(x)=f\left(W^TX \right)=f(\sum_{i=1}^{3}\omega_{i}x_{i}+b)$，其中f为被称为激活函数，可以认为是sigmoid函数，后面会介绍更多的激活函数。不难看出这样的一个神经元输入输出的映射关系正是之前推导过的逻辑回归。</p><h2 id="两层神经网络"><a href="#两层神经网络" class="headerlink" title="两层神经网络"></a>两层神经网络</h2><p>&emsp;&emsp;很多简单的神经元可以构成一张复杂的神经网络，为了方便演示神经网络的工作过程，我们以两层神经网络为例进行分析。两层神经网络的结构如下图所示：</p><div align="center"><br><img src="/浅层神经网络/两层神经网络.jpg" width="405" height="296" title="两层神经网络" align="center"><br></div><p>其中最左侧的是输入层，最右边的是输出层，中间的叫做隐含层，一般在说神经网络的层数时不算输入层，所以该模型是两层神经网络。该模型只包含一个隐含层，计算上较为简单，后面会根据该模型进行正向传播的推导。</p><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>&emsp;&emsp;根据两层神经网络的示意图进行正向传播的推导。首先假设$n_{l}$表示网络的层数，第l层记为$L_{l}$，所以本例中$L_{1}$是输入层，$L_{2}$是隐含层，$L_{3}$是输出层。</p><p>&emsp;&emsp;神经网络参数主要有$\left(W,b\right)$，其中$W^{(l)}_{ij}$表示第l层第j个单元和l+1层第i个单元之间的连接参数，即权重。$b^{(l)}_{i}$是第l+1层第i个单元的偏置项。</p><p>&emsp;&emsp;$a^{(l)}_{i}$表示第l层第i个单元的输出值。当l=1时，$a^{(1)}_{i}=x_{i}$，也就是第i个节点的输入值。最终的输出结果用$h_{W,b}(x)$表示。</p><p>&emsp;&emsp;所以该神经网络的正向传播过程如下。第一层到第二层的传播过程：</p><p>$$a^{(2)}_{1}=f\left[W^{(1)}_{11} x_{1}+W^{(1)}_{12} x_{2}+W^{(1)}_{13} x_{3}+b^{(1)}_{1}\right]$$<br>$$a^{(2)}_{2}=f\left[W^{(1)}_{21} x_{1}+W^{(1)}_{22} x_{2}+W^{(1)}_{23} x_{3}+b^{(1)}_{2}\right]$$<br>$$a^{(2)}_{3}=f\left[W^{(1)}_{31} x_{1}+W^{(1)}_{32} x_{2}+W^{(1)}_{33} x_{3}+b^{(1)}_{3}\right]$$</p><p>第二层到第三层的传播过程<br>$$a^{(3)}_{1}=f\left[W^{(2)}_{11} a^{(2)}_{1}+W^{(2)}_{12} a^{(2)}_{2}+W^{(2)}_{13} a^{(2)}_{3}+b^{(2)}_{1}\right]$$<br>$$h_{W,b}(x)=a^{(3)}_{1}$$</p><p>我们用$z^{l}_{i}$表示第l层第i单元的输入加权和，例如$z^{(2)}_{i}=\sum_{j=1}^{n}W^{(1)}_{ij}x_{j}+b^{(1)}_{i}$，则$a^{(l)}_{i}=f[z^{(l)}_{i}]$，则上述传播过程可以简化为<br>$$z^{(2)}=W^{(1)}x+b^{(1)}$$<br>$$a^{(2)}=f[z^{(2)}]$$<br>$$z^{(3)}=W^{(2)}a^{(2)}+b^{(2)}$$<br>$$h_{W,b}(x)=a^{(3)}=f[z^{(3)}]$$</p><p>上面的计算过程即为神经网络的正向传播过程。</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>&emsp;&emsp;正向传播计算出输出值$h_{W,b}(x)$后，根据与目标值$t$的误差，进行反向传播调整权重$W$和偏置$b$的值。根据平方和误差函数，针对第$p$个输入样本有$E=\frac{1}{2}\left|\left| h_{W,b} - t\right|\right|^2$，则总误差为$E_{N} = \frac{1}{N}\sum^{N}_{p=1}E_{p}$。误差从输出层反向传播，各层权重通过梯度下降算法进行更新，即$$w=w-\eta*\Delta E_{p}(w)$$其中，$\eta$是每次更新时的步长，$\Delta E_{p}(w)$是第$p$个样本的误差对某一层权重的偏导数。在这里以单输出两层神经网络为例，给出反向传播算法推导过程，为了简化公式，只考虑一个样本的情况。</p><p>&emsp;&emsp;对$w^{(1)}_{11},w^{(2)}_{11}$的推导。首先看第三层，输入是$z^{(3)}$，输出为$a^{(3)}$，误差为$E=\frac{1}{2}\left|\left|t - a^{(3)}\right|\right|^2$，有</p><p>$$\frac{\partial{E}}{\partial{w^{2}_{11}}}={\frac{\partial{E}}{\partial{z^{(3)}}}}{\frac{\partial{z^{(3)}}}{\partial{w^{(2)}_{11}}}}$$</p><p>对于${\frac{\partial{z^{(3)}}}{\partial{w^{(2)}_{11}}}}$,有</p><p>$${\frac{\partial{z^{(3)}}}{\partial{w^{(2)}_{11}}}}=a^{(2)}_{1}$$</p><p>对于${\frac{\partial{E}}{\partial{z^{(3)}}}}$，有</p><p>$${\frac{\partial{E}}{\partial{z^{(3)}}}} = {\frac{\partial{E}}{\partial{a^{(3)}}}}{\frac{\partial{a^{(3)}}}{\partial{z^{(3)}}}}=-(t-a^{(3)})*f’(z^{(3)})$$</p><p>若激活函数为simgoid函数，有<br>$${\frac{\partial{E}}{\partial{z^{(3)}}}} = -(t-a^{(3)})*z^{(3)}*(1-z^{(3)})$$<br>该结果在推导前一层权重$w$时会用到，令$\delta_{3}={\frac{\partial{E}}{\partial{z^{(3)}}}}$，代入$\frac{\partial{E}}{\partial{w^{2}_{11}}}$，有<br>$$\frac{\partial{E}}{\partial{w^{2}_{11}}} = \delta_{3}*a^{(2)}_{1}$$</p><p>&emsp;&emsp;对于第二层，需要求$\frac{\partial{E}}{\partial{w^{(1)}_{11}}}$，有<br>$$\frac{\partial{E}}{\partial{w^{(1)}_{11}}} = \frac{\partial{E}}{\partial{z^{(2)}_{1}}}\frac{\partial{z^{(2)}_{1}}}{\partial{w^{(1)}_{11}}}$$<br>其中<br>$$\frac{\partial{z^{(2)}_{1}}}{\partial{w^{(1)}_{11}}} = x_{1}$$<br>由于第三层只有一个节点，所以有<br>$$\frac{\partial{E}}{\partial{z^{(2)}_{1}}}= \frac{\partial{E}}{\partial{z^{(3)}_{1}}}\frac{\partial{z^{(3)}_{1}}}{\partial{z^{(2)}_{1}}}=\delta_{3}*\frac{\partial{z^{(3)}_{1}}}{\partial{a^{(2)}_{1}}}*\frac{\partial{a^{(2)}_{1}}}{\partial{z^{(2)}_{1}}}=\delta_{3}*w^{(2)}_{11}*z^{(2)}_{1}(1-z^{(2)}_{1})$$<br>令$\delta_{2}=\frac{\partial{E}}{\partial{z^{(2)}_{1}}}$，则有<br>$$\frac{\partial{E}}{\partial{w^{(1)}_{11}}}=\delta_{2}*x_{1}$$<br>其他的权重值与上述推导一致。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>&emsp;&emsp;激活函数的目的在于引入非线性，将输出映射到不同的空间。如果激活函数是线性的，那么多层次的神经网络和单层神经网络是等价的。比较常见的激活函数有simgoid函数、tanh函数、ReLU函数等。</p><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>&emsp;&emsp;sigmoid函数在logistic回归推导中给出了其函数图像及表达式，这里不再赘述。其优点在于能将输出映射在[0，1]区间，单调并连续。其缺点在于输入超过一定区间，函数的导数趋近于0，容易出现问题，并且其输出均值不为0.</p><h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>&emsp;&emsp;tanh函数被定义为$tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}$，其函数图像如下所示</p><div align="center"><br> <img src="/浅层神经网络/tanh图像.jpeg" width="400" height="300" title="tanh函数图像"><br></div><p>tanh函数相对于sigmoid函数来说，收敛速度更快，并且输出以0为中心，但是其依然没有解决梯度消失的问题。</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>&emsp;&emsp;RuLU的定义为$y=max(0,x)$，对应的函数图像为</p><div align="center"><br> <img src="/浅层神经网络/relu图像.jpg" width="336" height="225" title="relu函数图像"><br></div><p>relu函数可以取得较好的结果，在实际应用中也有很多类似的变种函数。relu函数有效缓解了梯度消失的问题，计算方面也比较容易。后面遇到例子时，再详细说明各种激活函数的效果。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>&emsp;&emsp;第三周的课程比较基础，通过两层神经网络结构进行了正向、反向传播推导，简单阐述了神经网络的工作原理，然而两层网络只是比较特殊的情况，推导不具有代表性，下一章深层网络将进行普适性的推导。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yajian.github.io/spark读取jar中文件的路径问题/"><span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="小建儿"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="小建儿的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a class="post-title-link" href="/spark读取jar中文件的路径问题/" itemprop="url">spark读取jar中文件的路径问题</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-28T20:21:15+08:00">2017-10-28</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/spark读取jar中文件的路径问题/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="spark读取jar中文件的路径问题/" itemprop="commentCount"></span></a></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span><span title="字数统计"></span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span><span title="阅读时长"></span></div></div></header><div class="post-body" itemprop="articleBody"><p>&emsp;&emsp;上周和这周的前两天一直在解决工作中遇到的一个问题，问题的背景是这样的：我们搭建的平台中使用了lucene的IKAnalyzer分词工具，该工具在进行分词时会先读取resource文件夹下的config文件，每次在本地跑ut测试的时候都能顺利通过，但是放到集群上执行的时候会报找不到config文件路径的错误。问题很清楚，但是就是不知道为什么无法找到文件，最后把路径信息打出来发现其中有个“！”，才知道问题的根结在于读取文件的方式。</p><p>&emsp;&emsp;先说说java中获取文件路径的几种方式，工程的目录结构如下</p><div align="center"><br> <img src="/spark读取jar中文件的路径问题/目录结构.jpg" width="300" height="380" tile="目录结构" align="center"><br></div><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> void testPath() &#123;</span><br><span class="line">       <span class="comment">// 第一种：获取类加载的根路径</span></span><br><span class="line">       URL url1 = <span class="keyword">this</span>.getClass().getResource(<span class="string">"/"</span>);</span><br><span class="line">       System.<span class="keyword">out</span>.println(url1.getPath());</span><br><span class="line">       <span class="comment">//输出：/Users/name/test/target/classes/</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">// 第二种：获取当前类的所在工程路径(不加“/”）     </span></span><br><span class="line">       URL url2 = <span class="keyword">this</span>.getClass().getResource(<span class="string">""</span>);</span><br><span class="line">       System.<span class="keyword">out</span>.println(url2.getPath());</span><br><span class="line">       <span class="comment">//输出：/Users/name/test/target/classes/test/</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">// 第三种：在资源加载的过程中，使用的逐级向上委托的形式加载，不能以“/”开头</span></span><br><span class="line">       URL url3 =<span class="keyword">this</span>.getClass().getClassLoader().getResource(<span class="string">""</span>);</span><br><span class="line">       System.<span class="keyword">out</span>.println(url3.getPath());</span><br><span class="line">       <span class="comment">//输出：file:/Users/name/test/target/classes/</span></span><br><span class="line">       URL url4 =<span class="keyword">this</span>.getClass().getClassLoader().getResource(<span class="string">"/"</span>);</span><br><span class="line">       System.<span class="keyword">out</span>.println(url3.getPath());</span><br><span class="line">       <span class="comment">//输出：null</span></span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;在正常情况下我们可以按照以上方法进行文件的读取，但是当我们把项目打成jar包，使用java -jar运行时，上述方法会在第一个打印方法出报空指针，这说明单独运行jar的时候上述读取文件的方法已经失效，但是我们采用下面的方法仍然可以进行文件的读取。</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> testStream() throws IOException &#123;</span><br><span class="line">        InputStream in = <span class="keyword">this</span>.getClass().getResourceAsStream(<span class="string">"/folder/3.txt"</span>);</span><br><span class="line">        <span class="keyword">int</span> temp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">byte</span>[] bytes = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">        <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">while</span> ((temp = in.<span class="built_in">read</span>()) != <span class="number">-1</span>) &#123;</span><br><span class="line">            bytes[len] = (<span class="keyword">byte</span>) temp;</span><br><span class="line">            len++;</span><br><span class="line">        &#125;</span><br><span class="line">        in.<span class="built_in">close</span>();</span><br><span class="line">        <span class="keyword">String</span> content = <span class="keyword">new</span> <span class="keyword">String</span>(bytes,<span class="number">0</span>,len);</span><br><span class="line">        System.out.<span class="built_in">println</span>(content);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>采用getResourceAsStream再运行打出来的jar包就没有问题了。</p><p>&emsp;&emsp;起初一直以为是spark运行jar的问题，后来才发现是java读取资源方式有错误。其实这个问题之前在进行web开发时遇到过，当时使用weblogic服务器，总是读取不到resource文件夹下的配置文件，后来也是采用getResourceAsStream的方法才读出来。然而在tomcat服务器下采用getResource的方式可以正常读取，因为tomcat服务器会解析放入的jar包，而weblogic却不会，运行机制不一样导致读取文件的方式也不一样。</p><p>&emsp;&emsp;解决完问题后想想这次的bug和之前遇到的本质是一个问题，之前遇到问题解决后也没有仔细思考，还以为是weblogic的特例，也就没有把经验积累下来。虽然这次后知后觉耽误了不少时间，但是印象更加深刻了，工作中遇到的坑还是应该记下来多看看，多总结。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a> <a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="小建儿"><p class="site-author-name" itemprop="name">小建儿</p><p class="site-description motion-element" itemprop="description">码农小白成长记</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives"><span class="site-state-item-count">50</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">11</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">17</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">小建儿</span></div><div class="powered-by"><i class="fa fa-user-md"></i> <span id="busuanzi_container_site_uv">本站访客数:<span id="busuanzi_value_site_uv"></span> |</span></div><div class="theme-info"><div class="powered-by"></div> <span class="post-count">| 博客全站共字</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script><script type="text/javascript" src="custom_mathjax_source"></script></body></html>